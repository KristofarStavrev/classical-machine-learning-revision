{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0076159",
   "metadata": {},
   "source": [
    "## GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b8d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92       106\n",
      "           1       0.91      0.89      0.90        94\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.91      0.91      0.91       200\n",
      "weighted avg       0.91      0.91      0.91       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,      # 1000 rows\n",
    "    n_features=20,       # 20 features\n",
    "    n_informative=15,    # 15 actually useful\n",
    "    n_redundant=5,       # 5 redundant\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Define the Gradient Boosting model\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=100,        # number of trees (iterations)\n",
    "    learning_rate=0.1,       # shrinkage of each tree's contribution\n",
    "    max_depth=3,             # depth of each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "939634e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.2940\n",
      "Mean Absolute Error (MAE): 0.3716\n",
      "R² Score: 0.7756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load the California housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 2. Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Initialize Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=100,      # number of trees\n",
    "    learning_rate=0.1,     # shrinkage\n",
    "    max_depth=3,           # depth of each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred):.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580d00c",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69285228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       106\n",
      "           1       0.80      0.82      0.81        94\n",
      "\n",
      "    accuracy                           0.82       200\n",
      "   macro avg       0.82      0.82      0.82       200\n",
      "weighted avg       0.82      0.82      0.82       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,      # 1000 rows\n",
    "    n_features=20,       # 20 features\n",
    "    n_informative=15,    # 15 actually useful\n",
    "    n_redundant=5,       # 5 redundant\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Define AdaBoost with a simple decision stump as the base estimator\n",
    "base_learner = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "model = AdaBoostClassifier(\n",
    "    estimator=base_learner,\n",
    "    n_estimators=50,         # number of weak learners\n",
    "    learning_rate=1.0,       # controls contribution of each learner\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96c14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.5682\n",
      "Mean Absolute Error (MAE): 0.6068\n",
      "R² Score: 0.5664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 1. Load California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 2. Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Define base learner\n",
    "base_learner = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "\n",
    "# 4. Create AdaBoost regressor model\n",
    "model = AdaBoostRegressor(\n",
    "    estimator=base_learner,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5. Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 7. Evaluate\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred):.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7fdae9",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd771b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9200\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       106\n",
      "           1       0.91      0.91      0.91        94\n",
      "\n",
      "    accuracy                           0.92       200\n",
      "   macro avg       0.92      0.92      0.92       200\n",
      "weighted avg       0.92      0.92      0.92       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,      # 1000 rows\n",
    "    n_features=20,       # 20 features\n",
    "    n_informative=15,    # 15 actually useful\n",
    "    n_redundant=5,       # 5 redundant\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create XGBoost classifier\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fcde3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.2273\n",
      "R^2 Score: 0.8266\n"
     ]
    }
   ],
   "source": [
    "# 1. Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 2. Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create XGBoost regressor\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# 4. Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f3df8",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8ce0999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9100\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       106\n",
      "           1       0.90      0.90      0.90        94\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.91      0.91      0.91       200\n",
      "weighted avg       0.91      0.91      0.91       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# 1. Generate synthetic binary classification data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 4. Set parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# 5. Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=100,\n",
    ")\n",
    "\n",
    "# 6. Predict class labels (threshold = 0.5)\n",
    "y_pred_proba = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# 7. Evaluate performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.4635\n",
      "R^2 Score: 0.8360\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=100\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"RMSE: {mse**0.5:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f28ad",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a520127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8900\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89        93\n",
      "           1       0.94      0.85      0.89       107\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.89      0.89      0.89       200\n",
      "weighted avg       0.89      0.89      0.89       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,      # 1000 rows\n",
    "    n_features=20,       # 20 features\n",
    "    n_informative=15,    # 15 actually useful\n",
    "    n_redundant=5,       # 5 redundant\n",
    "    random_state=42\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    verbose=False,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51d5fa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0934740\ttotal: 2.23ms\tremaining: 444ms\n",
      "100:\tlearn: 0.4867395\ttotal: 160ms\tremaining: 157ms\n",
      "199:\tlearn: 0.4323190\ttotal: 315ms\tremaining: 0us\n",
      "RMSE: 0.4799\n",
      "R^2 Score: 0.8243\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Load California Housing data\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoost regressor\n",
    "model = CatBoostRegressor(iterations=200, learning_rate=0.1, depth=6, verbose=100, random_seed=42)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"RMSE: {mse**0.5:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-beginner-projects-cA7GCgfX-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
